{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple Braeburn', 'Avocado', 'Blueberry', 'Lemon', 'Orange', 'Pear Forelle']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "##############################################\n",
    "learning_rate = 0.1  # initial learning rate\n",
    "min_learning_rate = 0.00001  # once the learning rate reaches this value, do not decrease it further\n",
    "learning_rate_reduction_factor = 0.5  # the factor used when reducing the learning rate -> learning_rate *= learning_rate_reduction_factor\n",
    "patience = 3  # how many epochs to wait before reducing the learning rate when the loss plateaus\n",
    "verbose = 1  # controls the amount of logging done during training and testing: 0 - none, 1 - reports metrics after each batch, 2 - reports metrics after each epoch\n",
    "image_size = (100, 100)  # width and height of the used images\n",
    "input_shape = (100, 100, 3)  # the expected input shape for the trained models; since the images in the Fruit-360 are 100 x 100 RGB images, this is the required input shape\n",
    "\n",
    "use_label_file = False  # set this to true if you want load the label names from a file; uses the label_file defined below; the file should contain the names of the used labels, each label on a separate line\n",
    "label_file = 'labels.txt'\n",
    "base_dir = ''  # relative path to the Fruit-Images-Dataset folder\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "train_dir = os.path.join(base_dir, 'Training')\n",
    "output_dir = 'output_files'  # root folder in which to save the the output files; the files will be under output_files/model_name \n",
    "##############################################\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if use_label_file:\n",
    "    with open(label_file, \"r\") as f:\n",
    "        labels = [x.strip() for x in f.readlines()]\n",
    "else:\n",
    "    labels = os.listdir(train_dir)\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create 2 charts, one for accuracy, one for loss, to show the evolution of these two metrics during the training process\n",
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1, len(model_history.history['accuracy']) + 1), len(model_history.history['accuracy']))\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1, len(model_history.history['loss']) + 1), len(model_history.history['loss']))\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later; the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# create a confusion matrix to visually represent incorrectly classified images\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Randomly changes hue and saturation of the image to simulate variable lighting conditions\n",
    "def augment_image(x):\n",
    "    import tensorflow as tf\n",
    "    x = tf.image.random_saturation(x, 0.9, 1.2)\n",
    "    x = tf.image.random_hue(x, 0.02)\n",
    "    return x\n",
    "\n",
    "# given the train and test folder paths and a validation to test ratio, this method creates three generators\n",
    "#  - the training generator uses (100 - validation_percent) of images from the train set \n",
    "#    it applies random horizontal and vertical flips for data augmentation and generates batches randomly\n",
    "#  - the validation generator uses the remaining validation_percent of images from the train set\n",
    "#    does not generate random batches, as the model is not trained on this data\n",
    "#    the accuracy and loss are monitored using the validation data so that the learning rate can be updated if the model hits a local optimum\n",
    "#  - the test generator uses the test set without any form of augmentation\n",
    "#    once the training process is done, the final values of accuracy and loss are calculated on this set\n",
    "def build_data_generators(train_folder, test_folder, validation_percent, labels=None, image_size=(100, 100), batch_size=50):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        preprocessing_function=augment_image, \n",
    "        validation_split=validation_percent)  # percentage indicating how much of the training set should be kept for validation\n",
    "\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                  batch_size=batch_size, shuffle=True, subset='training', classes=labels)\n",
    "    validation_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                       batch_size=batch_size, shuffle=False, subset='validation', classes=labels)\n",
    "    test_gen = test_datagen.flow_from_directory(test_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                batch_size=batch_size, shuffle=False, subset=None, classes=labels)\n",
    "    return train_gen, validation_gen, test_gen\n",
    "\n",
    "\n",
    "# this method performs all the steps from data setup, training and testing the model and plotting the results\n",
    "# the model is any trainable model; the input shape and output number of classes is dependant on the dataset used, in this case the input is 100x100 RGB images and the output is a softmax layer with 118 probabilities\n",
    "# the name is used to save the classification report containing the f1 score of the model, the plots showing the loss and accuracy and the confusion matrix\n",
    "# the batch size is used to determine the number of images passed through the network at once, the number of steps per epochs is derived from this as (total number of images in set // batch size) + 1\n",
    "def train_and_evaluate_model(model, name=\"\", epochs=25, batch_size=50, verbose=verbose, useCkpt=False):\n",
    "    print(model.summary())\n",
    "    model_out_dir = os.path.join(output_dir, name)\n",
    "    if not os.path.exists(model_out_dir):\n",
    "        os.makedirs(model_out_dir)\n",
    "    if useCkpt:\n",
    "        model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    trainGen, validationGen, testGen = build_data_generators(train_dir, test_dir, validation_percent=0.1, labels=labels, image_size=image_size, batch_size=batch_size)\n",
    "    optimizer = Adadelta(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=patience, verbose=verbose, \n",
    "                                                factor=learning_rate_reduction_factor, min_lr=min_learning_rate)\n",
    "    save_model = ModelCheckpoint(filepath=model_out_dir + \"/model.h5\", monitor='val_accuracy', verbose=verbose, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "    \n",
    "    history = model.fit(trainGen,\n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch=(trainGen.n // batch_size) + 1,\n",
    "                                  validation_data=validationGen,\n",
    "                                  validation_steps=(validationGen.n // batch_size) + 1,\n",
    "                                  verbose=verbose,\n",
    "                                  callbacks=[learning_rate_reduction, save_model])\n",
    "\n",
    "    model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    validationGen.reset()\n",
    "    loss_v, accuracy_v = model.evaluate(validationGen, steps=(validationGen.n // batch_size) + 1, verbose=verbose)\n",
    "    loss, accuracy = model.evaluate(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "    print(\"Test: accuracy = %f  ;  loss_v = %f\" % (accuracy, loss))\n",
    "    plot_model_history(history, out_path=model_out_dir)\n",
    "    testGen.reset()\n",
    "    y_pred = model.predict(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    y_true = testGen.classes[testGen.index_array]\n",
    "    plot_confusion_matrix(y_true, y_pred.argmax(axis=-1), labels, out_path=model_out_dir)\n",
    "    class_report = classification_report(y_true, y_pred.argmax(axis=-1), target_names=labels)\n",
    "\n",
    "    with open(model_out_dir + \"/classification_report.txt\", \"w\") as text_file:\n",
    "        text_file.write(\"%s\" % class_report)\n",
    "    # print(class_report)\n",
    "\n",
    "\n",
    "print(labels)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data (InputLayer)            [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100, 100, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 100, 100, 16)      1616      \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 50, 50, 32)        12832     \n",
      "_________________________________________________________________\n",
      "conv2_relu (Activation)      (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 25, 25, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv3_relu (Activation)      (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 12, 12, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv4_relu (Activation)      (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fcl1 (Dense)                 (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fcl2 (Dense)                 (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 5,254,198\n",
      "Trainable params: 5,254,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 2751 images belonging to 6 classes.\n",
      "Found 303 images belonging to 6 classes.\n",
      "Found 1019 images belonging to 6 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "56/56 [==============================] - 39s 562ms/step - loss: 6.4323 - accuracy: 0.5293 - val_loss: 0.0957 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.97360, saving model to output_files\\fruit-360 model\\model.h5\n",
      "Epoch 2/25\n",
      "56/56 [==============================] - 12s 213ms/step - loss: 0.4334 - accuracy: 0.8502 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.97360 to 1.00000, saving model to output_files\\fruit-360 model\\model.h5\n",
      "Epoch 3/25\n",
      "56/56 [==============================] - 12s 223ms/step - loss: 0.2921 - accuracy: 0.9015 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "Epoch 4/25\n",
      "56/56 [==============================] - 12s 220ms/step - loss: 0.1530 - accuracy: 0.9469 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 1.00000\n",
      "Epoch 5/25\n",
      "56/56 [==============================] - 13s 228ms/step - loss: 0.1499 - accuracy: 0.9618 - val_loss: 0.1360 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 1.00000\n",
      "Epoch 6/25\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.0493 - accuracy: 0.9873 - val_loss: 4.4719e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 1.00000\n",
      "Epoch 7/25\n",
      "56/56 [==============================] - 13s 226ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 8.8655e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 1.00000\n",
      "Epoch 8/25\n",
      "56/56 [==============================] - 14s 250ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 1.2349e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 1.00000\n",
      "Epoch 9/25\n",
      "56/56 [==============================] - 13s 225ms/step - loss: 0.0329 - accuracy: 0.9920 - val_loss: 7.7209e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 1.00000\n",
      "Epoch 10/25\n",
      "56/56 [==============================] - 13s 230ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 6.2505e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 1.00000\n",
      "Epoch 11/25\n",
      "56/56 [==============================] - 13s 225ms/step - loss: 0.0147 - accuracy: 0.9964 - val_loss: 8.9608e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 1.00000\n",
      "Epoch 12/25\n",
      "56/56 [==============================] - 13s 231ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 9.4396e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 1.00000\n",
      "Epoch 13/25\n",
      "56/56 [==============================] - 13s 239ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 3.7332e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 1.00000\n",
      "Epoch 14/25\n",
      "56/56 [==============================] - 14s 246ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.0229e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 1.00000\n",
      "Epoch 15/25\n",
      "56/56 [==============================] - 14s 259ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 9.0367e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 1.00000\n",
      "Epoch 16/25\n",
      "56/56 [==============================] - 16s 288ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 2.1583e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 1.00000\n",
      "Epoch 17/25\n",
      "56/56 [==============================] - 13s 227ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.1542e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 1.00000\n",
      "Epoch 18/25\n",
      "56/56 [==============================] - 14s 248ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 3.7167e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 1.00000\n",
      "Epoch 19/25\n",
      "56/56 [==============================] - 15s 269ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.8587e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 1.00000\n",
      "Epoch 20/25\n",
      "56/56 [==============================] - 15s 263ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 8.1551e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 1.00000\n",
      "Epoch 21/25\n",
      "56/56 [==============================] - 13s 227ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 4.5211e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 1.00000\n",
      "Epoch 22/25\n",
      "56/56 [==============================] - 13s 232ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 6.5510e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 1.00000\n",
      "Epoch 23/25\n",
      "56/56 [==============================] - 13s 233ms/step - loss: 8.6678e-04 - accuracy: 1.0000 - val_loss: 6.6597e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 1.00000\n",
      "Epoch 24/25\n",
      "56/56 [==============================] - 13s 228ms/step - loss: 5.1459e-04 - accuracy: 1.0000 - val_loss: 5.3399e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 1.00000\n",
      "Epoch 25/25\n",
      "56/56 [==============================] - 12s 220ms/step - loss: 9.9041e-04 - accuracy: 1.0000 - val_loss: 4.2025e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 1.00000\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "21/21 [==============================] - 7s 321ms/step - loss: 0.2248 - accuracy: 0.9519\n",
      "Validation: accuracy = 1.000000  ;  loss_v = 0.003334\n",
      "Test: accuracy = 0.951914  ;  loss_v = 0.224811\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_ticks() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e87f65b10221>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fruit-360 model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-6aa1d0a4f95d>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(model, name, epochs, batch_size, verbose, useCkpt)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation: accuracy = %f  ;  loss_v = %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test: accuracy = %f  ;  loss_v = %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[0mplot_model_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_out_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[0mtestGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-6aa1d0a4f95d>\u001b[0m in \u001b[0;36mplot_model_history\u001b[1;34m(model_history, out_path)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# summarize history for loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: set_ticks() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, Lambda\n",
    "\n",
    "\n",
    "# Create a custom layer that converts the original image from\n",
    "# RGB to HSV and grayscale and concatenates the results\n",
    "# forming in input of size 100 x 100 x 4\n",
    "def convert_to_hsv_and_grayscale(x):\n",
    "    import tensorflow as tf\n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    rez = tf.concat([hsv, gray], axis=-1)\n",
    "    return rez\n",
    "\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(convert_to_hsv_and_grayscale)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    rez = Model(inputs=img_input, outputs=out)\n",
    "    return rez\n",
    "\n",
    "\n",
    "model = network(input_shape=input_shape, num_classes=num_classes)\n",
    "train_and_evaluate_model(model, name=\"fruit-360 model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fruits-360 CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
